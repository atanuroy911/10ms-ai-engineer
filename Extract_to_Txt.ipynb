{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3af3b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All OCR dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Required imports for PDF text extraction\n",
    "import os\n",
    "import pandas as pd  # For timestamp in text files\n",
    "import fitz  # PyMuPDF for PDF handling\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "# OCR and text processing imports\n",
    "try:\n",
    "    import pytesseract\n",
    "    import cv2\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    print(\"‚úÖ All OCR dependencies imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Missing OCR dependencies: {e}\")\n",
    "    print(\"Please install with: pip install pytesseract opencv-python pillow numpy\")\n",
    "    print(\"Also install Tesseract OCR: https://github.com/UB-Mannheim/tesseract/wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ca0633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR and text processing functions\n",
    "import re\n",
    "import io\n",
    "\n",
    "def preprocess_image_for_ocr(image_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess image for better OCR results\n",
    "    \"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image_array.shape) == 3:\n",
    "        gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image_array\n",
    "    \n",
    "    # Increase contrast and brightness\n",
    "    alpha = 1.2  # Contrast control\n",
    "    beta = 10    # Brightness control\n",
    "    adjusted = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(adjusted, (1, 1), 0)\n",
    "    \n",
    "    # Apply threshold to get binary image\n",
    "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Morphological operations to clean up the image\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def pdf_page_to_image(pdf_path: str, page_num: int, dpi: int = 300) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert a specific PDF page to high-resolution image\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    # Create transformation matrix for high DPI\n",
    "    mat = fitz.Matrix(dpi/72, dpi/72)\n",
    "    \n",
    "    # Render page to pixmap\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    img_data = pix.tobytes(\"ppm\")\n",
    "    image = Image.open(io.BytesIO(img_data))\n",
    "    \n",
    "    doc.close()\n",
    "    return image\n",
    "\n",
    "def extract_text_with_ocr(pdf_path: str, page_num: int, dpi: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from PDF page using OCR\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert PDF page to image\n",
    "        image = pdf_page_to_image(pdf_path, page_num, dpi)\n",
    "        \n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Preprocess image for better OCR\n",
    "        processed_img = preprocess_image_for_ocr(img_array)\n",
    "        \n",
    "        # Convert back to PIL Image for pytesseract\n",
    "        pil_image = Image.fromarray(processed_img)\n",
    "        \n",
    "        # OCR configuration for Bengali\n",
    "        custom_config = r'--oem 3 --psm 6 -l ben+eng'  # Bengali + English\n",
    "        \n",
    "        # Extract text using OCR\n",
    "        text = pytesseract.image_to_string(pil_image, config=custom_config)\n",
    "        \n",
    "        return text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from page {page_num}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_bengali_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess Bengali text for better processing\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove common OCR artifacts\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\u0020-\\u007F\\u2000-\\u206F\\u2E00-\\u2E7F]', ' ', text)\n",
    "    \n",
    "    # Clean up punctuation spacing\n",
    "    text = re.sub(r'\\s+([‡•§,;:!?])', r'\\1', text)\n",
    "    text = re.sub(r'([‡•§,;:!?])\\s*', r'\\1 ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e0855fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bengali language support is available in Tesseract\n",
      "üìã Available languages: afr, amh, ara, asm, aze, aze_cyrl, bel, ben, bod, bos, bre, bul, cat, ceb, ces, chi_sim, chi_sim_vert, chi_tra, chi_tra_vert, chr, cos, cym, dan, deu, eng, equ, hun, hye, iku, ind, isl, ita, ita_old, jav, jpn, jpn_vert, kan, kat, kat_old, kaz, khm, kir, kor, osd, urd, uzb\n"
     ]
    }
   ],
   "source": [
    "# Tesseract OCR Configuration\n",
    "# Set the path to tesseract executable (Windows)\n",
    "# Adjust this path based on your Tesseract installation\n",
    "try:\n",
    "    # Common Windows path\n",
    "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "    \n",
    "    # Alternative Windows paths (uncomment if needed)\n",
    "    # pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\YourUsername\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "    \n",
    "    # For Linux/Mac, it's usually in PATH, so you might not need to set this\n",
    "    # pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "    \n",
    "    # Test if Bengali is available\n",
    "    available_langs = pytesseract.get_languages()\n",
    "    if 'ben' in available_langs:\n",
    "        print(\"‚úÖ Bengali language support is available in Tesseract\")\n",
    "    else:\n",
    "        print(\"‚ùå Bengali language support not found. Please install Bengali traineddata.\")\n",
    "        print(\"Download ben.traineddata from: https://github.com/tesseract-ocr/tessdata\")\n",
    "        print(\"Place it in your Tesseract tessdata directory\")\n",
    "    \n",
    "    # Show available languages\n",
    "    print(f\"üìã Available languages: {', '.join(available_langs)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Tesseract configuration issue: {e}\")\n",
    "    print(\"Please make sure Tesseract is properly installed and configured.\")\n",
    "    print(\"Install Tesseract from: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "    print(\"Or use: pip install tesseract (for some systems)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "979ccda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_text_to_files(\n",
    "    pdf_path: str = \"Data/HSC26-Bangla1st-Paper.pdf\",\n",
    "    start_page: Optional[int] = None,\n",
    "    end_page: Optional[int] = None,\n",
    "    dpi: int = 300,\n",
    "    output_dir: str = \"extracted_text\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract text from PDF using OCR and save to individual .txt files\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        start_page: Starting page number (0-indexed). If None, starts from beginning\n",
    "        end_page: Ending page number (0-indexed, inclusive). If None, goes to end\n",
    "        dpi: DPI for image conversion\n",
    "        output_dir: Directory to save text files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"üìÅ Created directory: {output_dir}\")\n",
    "    \n",
    "    # Open PDF to get page count\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)\n",
    "    doc.close()\n",
    "    \n",
    "    # Set page range\n",
    "    start_page = start_page if start_page is not None else 0\n",
    "    end_page = end_page if end_page is not None else total_pages - 1\n",
    "    \n",
    "    # Validate page range\n",
    "    start_page = max(0, min(start_page, total_pages - 1))\n",
    "    end_page = max(start_page, min(end_page, total_pages - 1))\n",
    "    \n",
    "    print(f\"üíæ Saving extracted text from pages {start_page + 1} to {end_page + 1}\")\n",
    "    print(f\"üìÑ PDF has {total_pages} total pages\")\n",
    "    \n",
    "    # Get base filename for naming\n",
    "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    # Extract and save text from each page\n",
    "    all_text = []\n",
    "    successful_pages = []\n",
    "    \n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        print(f\"Processing page {page_num + 1}/{total_pages}...\")\n",
    "        \n",
    "        # Extract text using OCR\n",
    "        raw_text = extract_text_with_ocr(pdf_path, page_num, dpi)\n",
    "        cleaned_text = preprocess_bengali_text(raw_text)\n",
    "        \n",
    "        if cleaned_text.strip():\n",
    "            # Save individual page text\n",
    "            page_filename = f\"{base_name}_page_{page_num + 1:03d}.txt\"\n",
    "            page_filepath = os.path.join(output_dir, page_filename)\n",
    "            \n",
    "            with open(page_filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Page {page_num + 1} - Raw Text:\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\")\n",
    "                f.write(raw_text)\n",
    "                f.write(\"\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "                f.write(f\"Page {page_num + 1} - Cleaned Text:\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\")\n",
    "                f.write(cleaned_text)\n",
    "                f.write(\"\\n\\n\" + \"=\" * 50 + \"\\n\")\n",
    "            \n",
    "            # all_text.append(f\"=== PAGE {page_num + 1} ===\\n{cleaned_text}\\n\")\n",
    "            all_text.append(f\"{cleaned_text}\\n\")\n",
    "            successful_pages.append(page_num + 1)\n",
    "            \n",
    "            print(f\"‚úÖ Saved: {page_filename} ({len(cleaned_text)} characters)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Page {page_num + 1}: No text extracted, skipping file creation\")\n",
    "    \n",
    "    # Save combined text file\n",
    "    if all_text:\n",
    "        combined_filename = f\"{base_name}_combined_pages_{start_page + 1}-{end_page + 1}.txt\"\n",
    "        combined_filepath = os.path.join(output_dir, combined_filename)\n",
    "        \n",
    "        with open(combined_filepath, 'w', encoding='utf-8') as f:\n",
    "            # f.write(f\"Combined Text from {pdf_path}\\n\")\n",
    "            # f.write(f\"Pages: {start_page + 1} to {end_page + 1}\\n\")\n",
    "            # f.write(f\"Extraction Date: {pd.Timestamp.now()}\\n\")\n",
    "            # f.write(f\"DPI: {dpi}\\n\")\n",
    "            # f.write(f\"Total Pages Processed: {len(successful_pages)}\\n\")\n",
    "            # f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            for text in all_text:\n",
    "                f.write(text + \"\\n\")\n",
    "                # f.write(\"\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        print(f\"\\nüìÑ Combined file saved: {combined_filename}\")\n",
    "        print(f\"üìä Successfully processed {len(successful_pages)} pages\")\n",
    "        print(f\"üìÅ All files saved in: {output_dir}\")\n",
    "        \n",
    "        # Create summary file\n",
    "        summary_filename = f\"{base_name}_extraction_summary.txt\"\n",
    "        summary_filepath = os.path.join(output_dir, summary_filename)\n",
    "        \n",
    "        with open(summary_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Text Extraction Summary\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            f.write(f\"Source PDF: {pdf_path}\\n\")\n",
    "            f.write(f\"Page Range: {start_page + 1} to {end_page + 1}\\n\")\n",
    "            f.write(f\"Total Pages in PDF: {total_pages}\\n\")\n",
    "            f.write(f\"Successfully Processed: {len(successful_pages)} pages\\n\")\n",
    "            f.write(f\"DPI Setting: {dpi}\\n\")\n",
    "            f.write(f\"Extraction Date: {pd.Timestamp.now()}\\n\")\n",
    "            f.write(f\"Output Directory: {output_dir}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Successfully Processed Pages:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for page in successful_pages:\n",
    "                f.write(f\"Page {page}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nFiles Created:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"- Individual page files: {len(successful_pages)} files\\n\")\n",
    "            f.write(f\"- Combined text file: {combined_filename}\\n\")\n",
    "            f.write(f\"- Summary file: {summary_filename}\\n\")\n",
    "        \n",
    "        print(f\"üìã Summary saved: {summary_filename}\")\n",
    "        \n",
    "        return successful_pages, output_dir\n",
    "    else:\n",
    "        print(\"‚ùå No text was extracted from any pages!\")\n",
    "        return [], output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce2bce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving extracted text from pages 6 to 20\n",
      "üìÑ PDF has 49 total pages\n",
      "Processing page 6/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_006.txt (1764 characters)\n",
      "Processing page 7/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_007.txt (1685 characters)\n",
      "Processing page 8/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_008.txt (2396 characters)\n",
      "Processing page 9/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_009.txt (1591 characters)\n",
      "Processing page 10/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_010.txt (1275 characters)\n",
      "Processing page 11/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_011.txt (1238 characters)\n",
      "Processing page 12/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_012.txt (2078 characters)\n",
      "Processing page 13/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_013.txt (1688 characters)\n",
      "Processing page 14/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_014.txt (1739 characters)\n",
      "Processing page 15/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_015.txt (2079 characters)\n",
      "Processing page 16/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_016.txt (917 characters)\n",
      "Processing page 17/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_017.txt (1202 characters)\n",
      "Processing page 18/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_018.txt (1584 characters)\n",
      "Processing page 19/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_019.txt (1632 characters)\n",
      "Processing page 20/49...\n",
      "‚úÖ Saved: HSC26-Bangla1st-Paper_page_020.txt (1248 characters)\n",
      "\n",
      "üìÑ Combined file saved: HSC26-Bangla1st-Paper_combined_pages_6-20.txt\n",
      "üìä Successfully processed 15 pages\n",
      "üìÅ All files saved in: extracted_text_bengali\n",
      "üìã Summary saved: HSC26-Bangla1st-Paper_extraction_summary.txt\n",
      "\n",
      "üéâ Text extraction completed!\n",
      "üìÅ Files saved in: extracted_text_bengali\n",
      "üìÑ Successfully processed: 15 pages\n"
     ]
    }
   ],
   "source": [
    "# üíæ Extract and save text to .txt files for backup and analysis\n",
    "# This will create individual page files and a combined file\n",
    "\n",
    "# Example 1: Save text from the same page range used for vector store\n",
    "successful_pages, output_dir = save_extracted_text_to_files(\n",
    "    pdf_path=\"Data/HSC26-Bangla1st-Paper.pdf\",\n",
    "    start_page=5,    # Same as vector store range\n",
    "    end_page=19,      # Same as vector store range\n",
    "    dpi=400,          # Same DPI for consistency\n",
    "    output_dir=\"extracted_text_bengali\"  # Custom directory name\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Text extraction completed!\")\n",
    "print(f\"üìÅ Files saved in: {output_dir}\")\n",
    "print(f\"üìÑ Successfully processed: {len(successful_pages)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "447483b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Additional utility functions for text file management\n",
    "\n",
    "def list_extracted_text_files(output_dir: str = \"extracted_text_bengali\"):\n",
    "    \"\"\"\n",
    "    List all extracted text files in the directory\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"‚ùå Directory {output_dir} does not exist\")\n",
    "        return\n",
    "    \n",
    "    files = [f for f in os.listdir(output_dir) if f.endswith('.txt')]\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"üìÅ No .txt files found in {output_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÑ Found {len(files)} text files in {output_dir}:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Separate different types of files\n",
    "    page_files = [f for f in files if '_page_' in f]\n",
    "    combined_files = [f for f in files if '_combined_' in f]\n",
    "    summary_files = [f for f in files if '_summary' in f]\n",
    "    \n",
    "    if page_files:\n",
    "        print(f\"\\nüìÑ Individual Page Files ({len(page_files)}):\")\n",
    "        for f in sorted(page_files):\n",
    "            file_path = os.path.join(output_dir, f)\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  - {f} ({size:,} bytes)\")\n",
    "    \n",
    "    if combined_files:\n",
    "        print(f\"\\nüìö Combined Files ({len(combined_files)}):\")\n",
    "        for f in combined_files:\n",
    "            file_path = os.path.join(output_dir, f)\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  - {f} ({size:,} bytes)\")\n",
    "    \n",
    "    if summary_files:\n",
    "        print(f\"\\nüìã Summary Files ({len(summary_files)}):\")\n",
    "        for f in summary_files:\n",
    "            file_path = os.path.join(output_dir, f)\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  - {f} ({size:,} bytes)\")\n",
    "\n",
    "def read_extracted_page(page_num: int, output_dir: str = \"extracted_text_bengali\", show_raw: bool = False):\n",
    "    \"\"\"\n",
    "    Read and display extracted text from a specific page\n",
    "    \"\"\"\n",
    "    # Find the page file\n",
    "    files = os.listdir(output_dir) if os.path.exists(output_dir) else []\n",
    "    page_file = None\n",
    "    \n",
    "    for f in files:\n",
    "        if f'_page_{page_num:03d}.txt' in f:\n",
    "            page_file = f\n",
    "            break\n",
    "    \n",
    "    if not page_file:\n",
    "        print(f\"‚ùå Page {page_num} text file not found in {output_dir}\")\n",
    "        return\n",
    "    \n",
    "    file_path = os.path.join(output_dir, page_file)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(f\"üìÑ Content from Page {page_num}:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if show_raw:\n",
    "        print(content)\n",
    "    else:\n",
    "        # Show only cleaned text\n",
    "        parts = content.split(\"Cleaned Text:\")\n",
    "        if len(parts) > 1:\n",
    "            cleaned_part = parts[1].split(\"=\" * 50)[0].strip()\n",
    "            print(cleaned_part[:1000] + \"...\" if len(cleaned_part) > 1000 else cleaned_part)\n",
    "        else:\n",
    "            print(content[:1000] + \"...\" if len(content) > 1000 else content)\n",
    "\n",
    "# Example usage:\n",
    "# list_extracted_text_files()\n",
    "# read_extracted_page(43)  # Read page 43 content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
