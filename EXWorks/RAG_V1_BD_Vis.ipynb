{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08085f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_vectorstore():\n",
    "    \"\"\"\n",
    "    Extract embeddings and metadata from the Bengali vector store\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the embedding model\n",
    "        embedding = FastEmbedEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        \n",
    "        # Load vector store\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=\"./bengali_chroma_db\", \n",
    "            embedding_function=embedding\n",
    "        )\n",
    "        \n",
    "        # Get all documents from the collection\n",
    "        collection = vector_store._collection\n",
    "        \n",
    "        # Retrieve all embeddings and metadata\n",
    "        results = collection.get(include=['embeddings', 'metadatas', 'documents'])\n",
    "        \n",
    "        embeddings = results['embeddings']\n",
    "        metadatas = results['metadatas']\n",
    "        documents = results['documents']\n",
    "        \n",
    "        print(f\"📊 Retrieved {len(embeddings)} embeddings\")\n",
    "        print(f\"📏 Embedding dimension: {len(embeddings[0]) if embeddings else 0}\")\n",
    "        \n",
    "        return embeddings, metadatas, documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error retrieving embeddings: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def visualize_embeddings_2d(method='umap', sample_size=None):\n",
    "    \"\"\"\n",
    "    Visualize embeddings in 2D using different dimensionality reduction techniques\n",
    "    \n",
    "    Args:\n",
    "        method: 'pca', 'tsne', or 'umap'\n",
    "        sample_size: Number of embeddings to sample (None for all)\n",
    "    \"\"\"\n",
    "    embeddings, metadatas, documents = get_embeddings_from_vectorstore()\n",
    "    \n",
    "    if embeddings is None:\n",
    "        print(\"❌ No embeddings found. Please run document ingestion first.\")\n",
    "        return\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    import numpy as np\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size and len(embeddings_array) > sample_size:\n",
    "        indices = np.random.choice(len(embeddings_array), sample_size, replace=False)\n",
    "        embeddings_array = embeddings_array[indices]\n",
    "        metadatas = [metadatas[i] for i in indices]\n",
    "        documents = [documents[i] for i in indices]\n",
    "    \n",
    "    print(f\"📈 Visualizing {len(embeddings_array)} embeddings using {method.upper()}\")\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    if method.lower() == 'pca':\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "        embeddings_2d = reducer.fit_transform(embeddings_array)\n",
    "        title = f\"PCA Visualization of Bengali Text Embeddings\"\n",
    "        \n",
    "    elif method.lower() == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings_array)-1))\n",
    "        embeddings_2d = reducer.fit_transform(embeddings_array)\n",
    "        title = f\"t-SNE Visualization of Bengali Text Embeddings\"\n",
    "        \n",
    "    elif method.lower() == 'umap':\n",
    "        reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, len(embeddings_array)-1))\n",
    "        embeddings_2d = reducer.fit_transform(embeddings_array)\n",
    "        title = f\"UMAP Visualization of Bengali Text Embeddings\"\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    df = pd.DataFrame({\n",
    "        'x': embeddings_2d[:, 0],\n",
    "        'y': embeddings_2d[:, 1],\n",
    "        'page': [meta.get('page', 'Unknown') for meta in metadatas],\n",
    "        'source': [meta.get('source', 'Unknown').split('/')[-1] for meta in metadatas],\n",
    "        'text_preview': [doc[:100] + '...' if len(doc) > 100 else doc for doc in documents],\n",
    "        'text_length': [len(doc) for doc in documents]\n",
    "    })\n",
    "    \n",
    "    # Create interactive plot with Plotly\n",
    "    fig = px.scatter(\n",
    "        df, \n",
    "        x='x', \n",
    "        y='y', \n",
    "        color='page',\n",
    "        size='text_length',\n",
    "        hover_data=['source', 'text_preview'],\n",
    "        title=title,\n",
    "        labels={'x': f'{method.upper()} Component 1', 'y': f'{method.upper()} Component 2'},\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(\n",
    "        hovertemplate='<b>Page:</b> %{color}<br>' +\n",
    "                      '<b>Source:</b> %{customdata[0]}<br>' +\n",
    "                      '<b>Text:</b> %{customdata[1]}<br>' +\n",
    "                      '<b>Length:</b> %{marker.size} chars<br>' +\n",
    "                      '<extra></extra>'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return df, embeddings_2d\n",
    "\n",
    "def analyze_embedding_clusters():\n",
    "    \"\"\"\n",
    "    Analyze embedding clusters and show statistics\n",
    "    \"\"\"\n",
    "    embeddings, metadatas, documents = get_embeddings_from_vectorstore()\n",
    "    \n",
    "    if embeddings is None:\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame({\n",
    "        'page': [meta.get('page', 'Unknown') for meta in metadatas],\n",
    "        'source': [meta.get('source', 'Unknown').split('/')[-1] for meta in metadatas],\n",
    "        'text_length': [len(doc) for doc in documents],\n",
    "        'word_count': [len(doc.split()) for doc in documents]\n",
    "    })\n",
    "    \n",
    "    print(\"📊 Embedding Collection Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total chunks: {len(df)}\")\n",
    "    print(f\"Pages covered: {df['page'].nunique()}\")\n",
    "    print(f\"Sources: {df['source'].nunique()}\")\n",
    "    print(f\"Average text length: {df['text_length'].mean():.0f} characters\")\n",
    "    print(f\"Average word count: {df['word_count'].mean():.0f} words\")\n",
    "    \n",
    "    print(\"\\n📄 Chunks per page:\")\n",
    "    page_counts = df['page'].value_counts().sort_index()\n",
    "    for page, count in page_counts.items():\n",
    "        print(f\"  Page {page}: {count} chunks\")\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Text length distribution\n",
    "    axes[0, 0].hist(df['text_length'], bins=20, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Distribution of Text Length')\n",
    "    axes[0, 0].set_xlabel('Characters')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Word count distribution\n",
    "    axes[0, 1].hist(df['word_count'], bins=20, alpha=0.7, color='lightgreen')\n",
    "    axes[0, 1].set_title('Distribution of Word Count')\n",
    "    axes[0, 1].set_xlabel('Words')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Chunks per page\n",
    "    page_counts.plot(kind='bar', ax=axes[1, 0], color='orange', alpha=0.7)\n",
    "    axes[1, 0].set_title('Chunks per Page')\n",
    "    axes[1, 0].set_xlabel('Page Number')\n",
    "    axes[1, 0].set_ylabel('Number of Chunks')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Text length vs word count scatter\n",
    "    axes[1, 1].scatter(df['word_count'], df['text_length'], alpha=0.6, color='red')\n",
    "    axes[1, 1].set_title('Text Length vs Word Count')\n",
    "    axes[1, 1].set_xlabel('Word Count')\n",
    "    axes[1, 1].set_ylabel('Character Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_query_similarity(query: str, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Visualize how a query relates to document embeddings\n",
    "    \"\"\"\n",
    "    embeddings, metadatas, documents = get_embeddings_from_vectorstore()\n",
    "    \n",
    "    if embeddings is None:\n",
    "        return\n",
    "    \n",
    "    # Load embedding model\n",
    "    embedding_model = FastEmbedEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    embeddings_array = np.array(embeddings)\n",
    "    query_array = np.array(query_embedding).reshape(1, -1)\n",
    "    \n",
    "    similarities = cosine_similarity(query_array, embeddings_array)[0]\n",
    "    \n",
    "    # Create DataFrame with similarities\n",
    "    df = pd.DataFrame({\n",
    "        'similarity': similarities,\n",
    "        'page': [meta.get('page', 'Unknown') for meta in metadatas],\n",
    "        'source': [meta.get('source', 'Unknown').split('/')[-1] for meta in metadatas],\n",
    "        'text_preview': [doc[:150] + '...' if len(doc) > 150 else doc for doc in documents],\n",
    "        'text_length': [len(doc) for doc in documents]\n",
    "    })\n",
    "    \n",
    "    # Sort by similarity\n",
    "    df = df.sort_values('similarity', ascending=False)\n",
    "    \n",
    "    print(f\"🔍 Query: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📊 Top {top_k} most similar chunks:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, row in df.head(top_k).iterrows():\n",
    "        print(f\"{i+1}. Similarity: {row['similarity']:.4f} | Page: {row['page']}\")\n",
    "        print(f\"   Text: {row['text_preview']}\")\n",
    "        print()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Similarity distribution\n",
    "    ax1.hist(similarities, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axvline(similarities.mean(), color='red', linestyle='--', label=f'Mean: {similarities.mean():.3f}')\n",
    "    ax1.axvline(np.percentile(similarities, 95), color='orange', linestyle='--', label='95th percentile')\n",
    "    ax1.set_title(f'Similarity Distribution for Query: \"{query}\"')\n",
    "    ax1.set_xlabel('Cosine Similarity')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top similarities by page\n",
    "    page_max_sim = df.groupby('page')['similarity'].max().sort_values(ascending=False)\n",
    "    page_max_sim.head(10).plot(kind='bar', ax=ax2, color='lightgreen', alpha=0.7)\n",
    "    ax2.set_title('Highest Similarity by Page')\n",
    "    ax2.set_xlabel('Page Number')\n",
    "    ax2.set_ylabel('Max Similarity Score')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df.head(top_k)\n",
    "\n",
    "def compare_embedding_methods():\n",
    "    \"\"\"\n",
    "    Compare different dimensionality reduction methods side by side\n",
    "    \"\"\"\n",
    "    embeddings, metadatas, documents = get_embeddings_from_vectorstore()\n",
    "    \n",
    "    if embeddings is None:\n",
    "        return\n",
    "    \n",
    "    import numpy as np\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    \n",
    "    # Sample if too many embeddings\n",
    "    if len(embeddings_array) > 200:\n",
    "        indices = np.random.choice(len(embeddings_array), 200, replace=False)\n",
    "        embeddings_array = embeddings_array[indices]\n",
    "        metadatas = [metadatas[i] for i in indices]\n",
    "        documents = [documents[i] for i in indices]\n",
    "    \n",
    "    # Apply different reduction methods\n",
    "    methods = {\n",
    "        'PCA': PCA(n_components=2, random_state=42),\n",
    "        't-SNE': TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings_array)-1)),\n",
    "        'UMAP': umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, len(embeddings_array)-1))\n",
    "    }\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=list(methods.keys()),\n",
    "        specs=[[{'type': 'scatter'}, {'type': 'scatter'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    colors = [meta.get('page', 0) for meta in metadatas]\n",
    "    \n",
    "    for i, (name, method) in enumerate(methods.items(), 1):\n",
    "        print(f\"Computing {name}...\")\n",
    "        reduced = method.fit_transform(embeddings_array)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=reduced[:, 0],\n",
    "                y=reduced[:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=colors,\n",
    "                    colorscale='viridis',\n",
    "                    size=8,\n",
    "                    opacity=0.7\n",
    "                ),\n",
    "                text=[f\"Page: {meta.get('page', 'Unknown')}<br>Text: {doc[:100]}...\" \n",
    "                      for meta, doc in zip(metadatas, documents)],\n",
    "                hovertemplate='%{text}<extra></extra>',\n",
    "                name=name\n",
    "            ),\n",
    "            row=1, col=i\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"Comparison of Dimensionality Reduction Methods\",\n",
    "        showlegend=False,\n",
    "        height=500,\n",
    "        width=1500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"✅ Comparison visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc43dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Analyze embedding collection statistics\n",
    "analyze_embedding_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Visualize embeddings in 2D using UMAP (recommended for large datasets)\n",
    "visualize_embeddings_2d(method='umap', sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8900174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Visualize how a specific query relates to document embeddings\n",
    "visualize_query_similarity(\"বিয়ের সময় কল্যাণীর বয়স\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Compare different dimensionality reduction methods side by side\n",
    "compare_embedding_methods()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
